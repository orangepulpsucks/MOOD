![MoodLogo](https://github.com/orangepulpsucks/AI_Expression_Detector/assets/42681894/4969656b-469f-4d9f-9f94-e76e6e21c6b9)

## Overview

'MOOD' is a React application that integrates Google's Vision API / GPT-4 / DALL-E 3 to detect emotion and generate an image of the user's current state. 
Click [here](https://www.youtube.com/watch?v=XFWfzq7Xrec) to see a demo.

This is Anyah and Yu-Ri's final project submission for CS330.
## Features
- [Google Vision API](https://cloud.google.com/vision?hl=en)
- [GPT-4](https://openai.com/gpt-4)
- [DALL-E 3](https://openai.com/dall-e-3)

## Run Docker
```
$ cd my-emotion-detection-app
$ npm run build
```


## Run React
```
$ cd my-emotion-detection-app
$ npm start
```


---

